-- Found pybind11: /home/liu4441/.conda/envs/liana/lib/python3.9/site-packages/pybind11/include (found version "3.0.1")
-- Configuring done (0.1s)
-- Generating done (0.0s)
-- Build files have been written to: /home/newdrive/liu4441/liana/pfor/standard/v17/build
[100%] Built target pforex_cpp
[RUN] Running benchmarks...
[CONFIG] 手动设定: 启动 1 个并行线程

>>> Running Config: llama2-7b-1 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-1==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 50.96% | Comp: 195.4 MB/s | Decomp: 401.4 MB/s | Metrics: {}

>>> Running Config: llama2-7b-2 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-2==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 68.11% | Comp: 166.7 MB/s | Decomp: 426.0 MB/s | Metrics: {}

>>> Running Config: llama2-7b-3 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-3==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 27.33% | Comp: 267.4 MB/s | Decomp: 400.3 MB/s | Metrics: {}

>>> Running Config: llama2-7b-4 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-4==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 41.89% | Comp: 179.5 MB/s | Decomp: 483.6 MB/s | Metrics: {}

>>> Running Config: llama2-7b-5 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-5==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Eval Error: shape '[32001, 4096]' is invalid for input of size 131072000
  -> Size: 61.96% | Comp: 316.8 MB/s | Decomp: 0.0 MB/s | Metrics: {}

>>> Running Config: llama2-7b-6 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-6==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Some parameters are on the meta device because they were offloaded to the cpu.
Eval Error: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 55.69 MiB is free. Including non-PyTorch memory, this process has 23.63 GiB memory in use. Of the allocated memory 23.29 GiB is allocated by PyTorch, and 49.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  -> Size: 18.38% | Comp: 361.7 MB/s | Decomp: 0.0 MB/s | Metrics: {}

>>> Running Config: llama2-7b-7 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-7==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 44.71% | Comp: 343.6 MB/s | Decomp: 380.6 MB/s | Metrics: {}

>>> Running Config: llama2-7b-8 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-8==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 26.09% | Comp: 197.0 MB/s | Decomp: 423.2 MB/s | Metrics: {}

>>> Running Config: llama2-7b-9 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-9==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 50.96% | Comp: 209.5 MB/s | Decomp: 406.6 MB/s | Metrics: {}

>>> Running Config: llama2-7b-10 <<<
Detecting FT model native dtype...

[========== Rate: 0.0 = Model: llama2-7b-10==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[CONFIG] Skipping LM-Harness tasks (List is empty).
  -> Size: 31.32% | Comp: 180.7 MB/s | Decomp: 415.3 MB/s | Metrics: {}

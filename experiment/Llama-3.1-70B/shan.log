-- Found pybind11: /home/liu4441/.conda/envs/liana/lib/python3.9/site-packages/pybind11/include (found version "3.0.1")
-- Configuring done (0.2s)
-- Generating done (0.0s)
-- Build files have been written to: /home/newdrive/liu4441/liana/pfor/standard/v17/build
[100%] Built target pforex_cpp
[RUN] Running benchmarks...
[CONFIG] 手动设定: 启动 1 个并行线程

>>> Running Config: llama-3.1-70b-1 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama-3.1-70b-1==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 31.67% | Comp: 316.5 MB/s | Decomp: 386.7 MB/s | Metrics: {}

>>> Running Config: llama-3.1-70b-2 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama-3.1-70b-2==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 28.72% | Comp: 328.6 MB/s | Decomp: 397.9 MB/s | Metrics: {}

>>> Running Config: llama-3.1-70b-3 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama-3.1-70b-3==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 31.67% | Comp: 315.4 MB/s | Decomp: 397.4 MB/s | Metrics: {}

>>> Running Config: llama-3.1-70b-4 <<<
Detecting FT model native dtype...
Dtype detection failed: mlabonne/Meta-Llama-3.1-70B-Instruct-abliterated is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`. Defaulting to float32.

[========== Rate: 0.0 = Model: llama-3.1-70b-4==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
!!! CRITICAL ERROR processing llama-3.1-70b-4 !!!
mlabonne/Meta-Llama-3.1-70B-Instruct-abliterated is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

>>> Running Config: llama-3.1-70b-5 <<<
Detecting FT model native dtype...
Dtype detection failed: arcee-ai/SuperNova-Pro is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`. Defaulting to float32.

[========== Rate: 0.0 = Model: llama-3.1-70b-5==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
!!! CRITICAL ERROR processing llama-3.1-70b-5 !!!
arcee-ai/SuperNova-Pro is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

>>> Running Config: llama-3.1-70b-6 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama-3.1-70b-6==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 31.67% | Comp: 316.7 MB/s | Decomp: 384.1 MB/s | Metrics: {}

>>> Running Config: llama-3.1-70b-7 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.float32

[========== Rate: 0.0 = Model: llama-3.1-70b-7==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625506816 bytes required
  - 1: 4202889216 bytes required
  - 2: 4202889216 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 73.35% | Comp: 200.3 MB/s | Decomp: 411.2 MB/s | Metrics: {}

>>> Running Config: llama-3.1-70b-8 <<<
Detecting FT model native dtype...
Dtype detection failed: DeepMount00/Llama-3.1-70b-Ita is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`. Defaulting to float32.

[========== Rate: 0.0 = Model: llama-3.1-70b-8==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
!!! CRITICAL ERROR processing llama-3.1-70b-8 !!!
DeepMount00/Llama-3.1-70b-Ita is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

>>> Running Config: llama-3.1-70b-9 <<<
Detecting FT model native dtype...
Dtype detection failed: elyza/ELYZA-japanese-Llama-3.1-70B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`. Defaulting to float32.

[========== Rate: 0.0 = Model: llama-3.1-70b-9==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
!!! CRITICAL ERROR processing llama-3.1-70b-9 !!!
elyza/ELYZA-japanese-Llama-3.1-70B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`

>>> Running Config: llama-3.1-70b-10 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama-3.1-70b-10==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 7625310208 bytes required
  - 1: 4202692608 bytes required
  - 2: 4202692608 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Some parameters are on the meta device because they were offloaded to the cpu and disk.
  -> Size: 31.69% | Comp: 316.4 MB/s | Decomp: 389.1 MB/s | Metrics: {}


test_list = [
    "meta-llama/Llama-3.1-70B-Instruct",
    "NousResearch/Hermes-3-Llama-3.1-70B",
    "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
    "unsloth/Meta-Llama-3.1-70B-Instruct",
    "mattshumer/Reflection-Llama-3.1-70B",
    "VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct"
]

    "mlabonne/Meta-Llama-3.1-70B-Instruct-abliterated",
    "arcee-ai/SuperNova-Pro",
    "DeepMount00/Llama-3.1-70b-Ita",
    "elyza/ELYZA-japanese-Llama-3.1-70B-Instruct",
    "cognitivecomputations/dolphin-2.9.4-llama3.1-70b",
    "akjindal53244/Llama-3.1-Storm-70B",
    "failspy/Llama-3.1-70B-Instruct-Abliterated",
    "Magpie-Align/Llama-3.1-70B-Magpie-Align-SFT-v0.2",
    "rombodawg/Llama-3.1-70B-Instruct-Uncensored",


    
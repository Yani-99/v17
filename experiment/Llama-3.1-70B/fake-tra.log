
=====================================================================================================================================
Model Name                                                   | Method   | Orig(MB)   | Comp%      | C-Speed(MB/s)   | D-Speed(MB/s)  
-------------------------------------------------------------------------------------------------------------------------------------
meta-llama/Llama-3.1-70B-Instruct                            | zlib     | 134570.6 |    78.56% |         14.65 |        247.66
meta-llama/Llama-3.1-70B-Instruct                            | gzip     | 134570.6 |    78.56% |         14.65 |        247.66
meta-llama/Llama-3.1-70B-Instruct                            | zstd     | 134570.6 |    76.99% |        477.96 |        987.79
NousResearch/Hermes-3-Llama-3.1-70B                          | zlib     | 134570.6 |    79.08% |         16.47 |        271.97
NousResearch/Hermes-3-Llama-3.1-70B                          | gzip     | 134570.6 |    79.08% |         16.47 |        271.97
NousResearch/Hermes-3-Llama-3.1-70B                          | zstd     | 134570.6 |    77.78% |        531.31 |        756.49
nvidia/Llama-3.1-Nemotron-70B-Instruct-HF                    | zlib     | 134570.6 |    78.85% |         15.27 |        259.28
nvidia/Llama-3.1-Nemotron-70B-Instruct-HF                    | gzip     | 134570.6 |    78.85% |         15.27 |        259.28
nvidia/Llama-3.1-Nemotron-70B-Instruct-HF                    | zstd     | 134570.6 |    77.46% |        417.20 |        580.66
unsloth/Meta-Llama-3.1-70B-Instruct                          | zlib     | 134570.6 |    78.56% |         14.66 |        246.34
unsloth/Meta-Llama-3.1-70B-Instruct                          | gzip     | 134570.6 |    78.56% |         14.66 |        246.34
unsloth/Meta-Llama-3.1-70B-Instruct                          | zstd     | 134570.6 |    76.99% |        385.80 |        623.97
mattshumer/Reflection-Llama-3.1-70B                          | zlib     | 269141.5 |    91.30% |         31.95 |        309.09
mattshumer/Reflection-Llama-3.1-70B                          | gzip     | 269141.5 |    91.30% |         31.95 |        309.09
mattshumer/Reflection-Llama-3.1-70B                          | zstd     | 269141.5 |    91.11% |        430.49 |        616.83
VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct            | zlib     | 134570.6 |    78.64% |         14.92 |        249.90
VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct            | gzip     | 134570.6 |    78.64% |         14.92 |        249.90
VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct            | zstd     | 134570.6 |    77.12% |        395.97 |        601.30
allenai/Llama-3.1-Tulu-3-70B-SFT                             | zlib     | 134570.8 |    79.08% |         16.50 |        271.15
allenai/Llama-3.1-Tulu-3-70B-SFT                             | gzip     | 134570.8 |    79.08% |         16.50 |        271.15
allenai/Llama-3.1-Tulu-3-70B-SFT                             | zstd     | 134570.8 |    77.78% |        424.95 |        513.52
nvidia/OpenMath2-Llama3.1-70B                                | zlib     | 134570.6 |    78.28% |         14.35 |        242.04
nvidia/OpenMath2-Llama3.1-70B                                | gzip     | 134570.6 |    78.28% |         14.35 |        242.04
nvidia/OpenMath2-Llama3.1-70B                                | zstd     | 134570.6 |    76.66% |        373.21 |        659.92
migtissera/Tess-3-Llama-3.1-70B                              | zlib     | 134570.8 |    79.08% |         16.32 |        266.71
migtissera/Tess-3-Llama-3.1-70B                              | gzip     | 134570.8 |    79.08% |         16.32 |        266.71
migtissera/Tess-3-Llama-3.1-70B                              | zstd     | 134570.8 |    77.79% |        516.33 |        686.29
mylesgoose/Llama-3.1-70B-Instruct-abliterated                | zlib     | 134570.6 |    78.72% |         15.18 |        252.28
mylesgoose/Llama-3.1-70B-Instruct-abliterated                | gzip     | 134570.6 |    78.72% |         15.18 |        252.28
mylesgoose/Llama-3.1-70B-Instruct-abliterated                | zstd     | 134570.6 |    77.25% |        413.84 |        680.96

==============================
AGGREGATED RESULTS BY METHOD
==============================

Method: zstd
{
    "meta-llama/Llama-3.1-70B-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 76.99,
        "CompSpeed": 477.96,
        "DecompSpeed": 987.79
    },
    "NousResearch/Hermes-3-Llama-3.1-70B": {
        "OrigMB": 134570.6,
        "CompRatio": 77.78,
        "CompSpeed": 531.31,
        "DecompSpeed": 756.49
    },
    "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
        "OrigMB": 134570.6,
        "CompRatio": 77.46,
        "CompSpeed": 417.2,
        "DecompSpeed": 580.66
    },
    "unsloth/Meta-Llama-3.1-70B-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 76.99,
        "CompSpeed": 385.8,
        "DecompSpeed": 623.97
    },
    "mattshumer/Reflection-Llama-3.1-70B": {
        "OrigMB": 269141.49,
        "CompRatio": 91.11,
        "CompSpeed": 430.49,
        "DecompSpeed": 616.83
    },
    "VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 77.12,
        "CompSpeed": 395.97,
        "DecompSpeed": 601.3
    },
    "allenai/Llama-3.1-Tulu-3-70B-SFT": {
        "OrigMB": 134570.85,
        "CompRatio": 77.78,
        "CompSpeed": 424.95,
        "DecompSpeed": 513.52
    },
    "nvidia/OpenMath2-Llama3.1-70B": {
        "OrigMB": 134570.6,
        "CompRatio": 76.66,
        "CompSpeed": 373.21,
        "DecompSpeed": 659.92
    },
    "migtissera/Tess-3-Llama-3.1-70B": {
        "OrigMB": 134570.77,
        "CompRatio": 77.79,
        "CompSpeed": 516.33,
        "DecompSpeed": 686.29
    },
    "mylesgoose/Llama-3.1-70B-Instruct-abliterated": {
        "OrigMB": 134570.6,
        "CompRatio": 77.25,
        "CompSpeed": 413.84,
        "DecompSpeed": 680.96
    }
}

Method: gzip
{
    "meta-llama/Llama-3.1-70B-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 78.56,
        "CompSpeed": 14.65,
        "DecompSpeed": 247.66
    },
    "NousResearch/Hermes-3-Llama-3.1-70B": {
        "OrigMB": 134570.6,
        "CompRatio": 79.08,
        "CompSpeed": 16.47,
        "DecompSpeed": 271.97
    },
    "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF": {
        "OrigMB": 134570.6,
        "CompRatio": 78.85,
        "CompSpeed": 15.27,
        "DecompSpeed": 259.28
    },
    "unsloth/Meta-Llama-3.1-70B-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 78.56,
        "CompSpeed": 14.66,
        "DecompSpeed": 246.34
    },
    "mattshumer/Reflection-Llama-3.1-70B": {
        "OrigMB": 269141.49,
        "CompRatio": 91.3,
        "CompSpeed": 31.95,
        "DecompSpeed": 309.09
    },
    "VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct": {
        "OrigMB": 134570.6,
        "CompRatio": 78.64,
        "CompSpeed": 14.92,
        "DecompSpeed": 249.9
    },
    "allenai/Llama-3.1-Tulu-3-70B-SFT": {
        "OrigMB": 134570.85,
        "CompRatio": 79.08,
        "CompSpeed": 16.5,
        "DecompSpeed": 271.15
    },
    "nvidia/OpenMath2-Llama3.1-70B": {
        "OrigMB": 134570.6,
        "CompRatio": 78.28,
        "CompSpeed": 14.35,
        "DecompSpeed": 242.04
    },
    "migtissera/Tess-3-Llama-3.1-70B": {
        "OrigMB": 134570.77,
        "CompRatio": 79.08,
        "CompSpeed": 16.32,
        "DecompSpeed": 266.71
    },
    "mylesgoose/Llama-3.1-70B-Instruct-abliterated": {
        "OrigMB": 134570.6,
        "CompRatio": 78.72,
        "CompSpeed": 15.18,
        "DecompSpeed": 252.28
    }
}

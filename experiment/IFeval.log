-- Found pybind11: /home/liu4441/.local/lib/python3.9/site-packages/pybind11/include (found version "3.0.2")
-- Configuring done (0.1s)
-- Generating done (0.0s)
-- Build files have been written to: /home/newdrive/liu4441/liana/pfor/standard/v17/build
[100%] Built target pforex_cpp
[RUN] Running benchmarks...
[CONFIG] 手动设定: 启动 1 个并行线程

>>> Running Config: llama3.1-8b-1 <<<
Detecting FT model native dtype...
-> Found dtype in config: torch.bfloat16

[========== Rate: 0.0 = Model: llama3.1-8b-1==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu', 'gsm8k', 'ifeval']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Downloaded punkt_tab on rank 0
ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}
gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
Building contexts for ifeval on rank 0...
Building contexts for gsm8k on rank 0...
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running generate_until requests
Running loglikelihood requests
  -> Size: 44.31% | Comp: 383.8 MB/s | Decomp: 406.8 MB/s | Metrics: {'mmlu/acc': 0.637872097991739, 'mmlu_humanities/acc': 0.5819341126461212, 'mmlu_other/acc': 0.7183778564531703, 'mmlu_social_sciences/acc': 0.7400064998375041, 'mmlu_stem/acc': 0.5423406279733587, 'ifeval/prompt_strict_acc': 0.4011090573012939, 'ifeval/inst_strict_acc': 0.5455635491606715, 'ifeval/prompt_loose_acc': 0.4787430683918669, 'ifeval/inst_loose_acc': 0.6091127098321343, 'gsm8k/acc': 0.7573919636087946}

[========== Rate: 0.0001 = Model: llama3.1-8b-1==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu', 'gsm8k', 'ifeval']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}
gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
Building contexts for ifeval on rank 0...
Building contexts for gsm8k on rank 0...
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running generate_until requests
Running loglikelihood requests
  -> Size: 44.31% | Comp: 274.5 MB/s | Decomp: 415.2 MB/s | Metrics: {'mmlu/acc': 0.638085742771685, 'mmlu_humanities/acc': 0.5823591923485654, 'mmlu_other/acc': 0.7196652719665272, 'mmlu_social_sciences/acc': 0.7396815079623009, 'mmlu_stem/acc': 0.5417063114494133, 'ifeval/prompt_strict_acc': 0.43068391866913125, 'ifeval/inst_strict_acc': 0.5587529976019184, 'ifeval/prompt_loose_acc': 0.5101663585951941, 'ifeval/inst_loose_acc': 0.6270983213429256, 'gsm8k/acc': 0.7498104624715694}

[========== Rate: 0.0005 = Model: llama3.1-8b-1==========]
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:
  - 0: 2101346304 bytes required
These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu', 'gsm8k', 'ifeval']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}
gsm8k: Using gen_kwargs: {'until': ['Question:', '</s>', '<|im_end|>'], 'do_sample': False, 'temperature': 0.0}
Building contexts for ifeval on rank 0...
Building contexts for gsm8k on rank 0...
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running generate_until requests

import os
import sys
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ¨æ€æ·»åŠ ä½ é¡¹ç›®ä¸­çš„ bench/modules åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿ç›´æ¥å¤ç”¨ä½ çš„è¯„ä¼°ä»£ç 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../bench/modules")))
from evaluator import UniversalEvaluator

def evaluate_gptq_model():
    # ä½ çš„ GPTQ æ¨¡å‹ç»å¯¹è·¯å¾„
    model_path = "/home/newdrive2/liu4441/Llama-2-7b-chat-hf-gptq-4bit"
    
    print(f"[*] å¼€å§‹åŠ è½½ Tokenizer: {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    print(f"[*] å¼€å§‹åŠ è½½ GPTQ æ¨¡å‹ (è‡ªåŠ¨åˆ†é…æ˜¾å­˜)...")
    # GPTQ æ¨¡å‹çš„åŠ è½½åªéœ€è¦ device_map="auto" å³å¯ï¼Œæ— éœ€ä¼  torch_dtype
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto"
    )
    
    # ==========================================
    # æ„é€ ä¸ä½ çš„ configs.py å®Œå…¨ä¸€è‡´çš„é…ç½®å­—å…¸
    # ==========================================
    cfg = {
        "task_type": "LLM_HARNESS",
        "ft_model": "meta-llama/Llama-2-7b-chat-hf", # ä¿æŒä½ çš„åŸæ¨¡å‹åç§°ï¼Œä»¥ä¾¿å¯èƒ½å­˜åœ¨çš„æ¨¡æ¿åŒ¹é…
        "llm_tasks": ["mmlu", "gsm8k"],  # è¿™é‡Œå†™ä¸Šä½ éœ€è¦å¯¹æ¯”çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ ["mmlu"] æˆ– ["wikitext"]
        "run_mt_bench": True,           # æ˜¯å¦é¡ºå¸¦è·‘ä½ ä»£ç é‡Œçš„ MT-Bench
        "eval_limit": None               # è®¾ä¸ºå…·ä½“æ•°å­—(å¦‚100)å¯ç”¨äºå¿«é€Ÿ debug æµ‹è¯•ï¼Œè·‘å…¨é‡è®¾ä¸º None
    }
    
    print(f"[*] å¼€å§‹ä½¿ç”¨é¡¹ç›®çš„ UniversalEvaluator è¿›è¡Œå¯¹é½è¯„æµ‹...")
    print(f"[*] è¯„æµ‹ä»»åŠ¡: {cfg['llm_tasks']}")
    
    # å®ä¾‹åŒ–ä½ è‡ªå·±çš„è¯„æµ‹å™¨
    evaluator = UniversalEvaluator(
        model=model, 
        processor=tokenizer, 
        config=cfg, 
        run_tag="gptq-4bit-baseline", 
        device="cuda"
    )
    
    # è¿è¡Œè¯„æµ‹ï¼Œå®ƒä¼šèµ°ä½  _run_lm_harness() é‡Œé¢çš„å…¨éƒ¨é€»è¾‘
    metrics = evaluator.run()
    
    # æ‰“å°æœ€ç»ˆä½ éœ€è¦çš„ã€å¯ä»¥ç›´æ¥å¡«åˆ°ä½ è®ºæ–‡è¡¨æ ¼é‡Œçš„ç»“æœ
    print("\n" + "="*60)
    print(" ğŸ“Š GPTQ 4-bit æ¨¡å‹è¯„æµ‹ç»“æœ (å¯¹é½é¡¹ç›®æ ‡å‡†)")
    print("="*60)
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f" {k:<20}: {v:.4f}")
        else:
            print(f" {k:<20}: {v}")
    print("="*60)

if __name__ == "__main__":
    evaluate_gptq_model()
import os
import sys
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ¨æ€æ·»åŠ ä½ é¡¹ç›®ä¸­çš„ bench/modules åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿ç›´æ¥å¤ç”¨ä½ çš„è¯„ä¼°ä»£ç 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../bench/modules")))
from evaluator import UniversalEvaluator

def evaluate_gptq_model():
    # ä½ çš„ GPTQ æ¨¡å‹ç»å¯¹è·¯å¾„
    model_path = "/home/newdrive2/liu4441/Llama-3.1-8B-Instruct-gptq-4bit"
    
    print(f"[*] å¼€å§‹åŠ è½½ Tokenizer: {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    tokenizer.padding_side = "left"  # æ‰¹é‡ç”Ÿæˆå¿…é¡»å·¦å¡«å……
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # å¼ºåˆ¶ç»‘å®šåˆ¹è½¦ç¬¦

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    model.generation_config.eos_token_id = terminators
    
    print(f"[*] å¼€å§‹åŠ è½½ GPTQ æ¨¡å‹ (è‡ªåŠ¨åˆ†é…æ˜¾å­˜)...")
    # GPTQ æ¨¡å‹çš„åŠ è½½åªéœ€è¦ device_map="auto" å³å¯ï¼Œæ— éœ€ä¼  torch_dtype
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map={"": 0},
        torch_dtype=torch.float16
    )
    
    # ==========================================
    # æ„é€ ä¸ä½ çš„ configs.py å®Œå…¨ä¸€è‡´çš„é…ç½®å­—å…¸
    # ==========================================
    cfg = {
        "task_type": "LLM_HARNESS",
        "base_model": "meta-llama/Llama-3.1-8B",
        "ft_model": "meta-llama/Llama-3.1-8B-Instruct", # ä¿æŒä½ çš„åŸæ¨¡å‹åç§°ï¼Œä»¥ä¾¿å¯èƒ½å­˜åœ¨çš„æ¨¡æ¿åŒ¹é…
        "llm_tasks": ["mmlu", "gsm8k","ifeval"],  # è¿™é‡Œå†™ä¸Šä½ éœ€è¦å¯¹æ¯”çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ ["mmlu"] æˆ– ["wikitext"]
        "run_mt_bench": True,           # æ˜¯å¦é¡ºå¸¦è·‘ä½ ä»£ç é‡Œçš„ MT-Bench
        "eval_limit": None,               # è®¾ä¸ºå…·ä½“æ•°å­—(å¦‚100)å¯ç”¨äºå¿«é€Ÿ debug æµ‹è¯•ï¼Œè·‘å…¨é‡è®¾ä¸º None
        "batch_size": "32"
    }

    print(f"[*] å¼€å§‹ä½¿ç”¨é¡¹ç›®çš„ UniversalEvaluator è¿›è¡Œå¯¹é½è¯„æµ‹...")
    print(f"[*] è¯„æµ‹ä»»åŠ¡: {cfg['llm_tasks']}")
    
    # å®ä¾‹åŒ–ä½ è‡ªå·±çš„è¯„æµ‹å™¨
    evaluator = UniversalEvaluator(
        model=model, 
        processor=tokenizer, 
        config=cfg, 
        run_tag="gptq-4bit-baseline", 
        device="cuda"
    )
    
    # è¿è¡Œè¯„æµ‹ï¼Œå®ƒä¼šèµ°ä½  _run_lm_harness() é‡Œé¢çš„å…¨éƒ¨é€»è¾‘
    metrics = evaluator.run()
    
    # æ‰“å°æœ€ç»ˆä½ éœ€è¦çš„ã€å¯ä»¥ç›´æ¥å¡«åˆ°ä½ è®ºæ–‡è¡¨æ ¼é‡Œçš„ç»“æœ
    print("\n" + "="*60)
    print(" ğŸ“Š GPTQ 4-bit æ¨¡å‹è¯„æµ‹ç»“æœ (å¯¹é½é¡¹ç›®æ ‡å‡†)")
    print("="*60)
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f" {k:<20}: {v:.4f}")
        else:
            print(f" {k:<20}: {v}")
    print("="*60)

if __name__ == "__main__":
    evaluate_gptq_model()
[*] å¼€å§‹åŠ è½½ Tokenizer: /home/newdrive2/liu4441/Llama-3.1-8B-Instruct-gptq-4bit...
[*] å¼€å§‹åŠ è½½ GPTQ æ¨¡å‹ (è‡ªåŠ¨åˆ†é…æ˜¾å­˜)...

[33mWARN[0m  Python GIL is enabled: Multi-gpu quant acceleration for MoE models is sub-optimal and multi-core accelerated cpu packing is also disabled. We recommend Python >= 3.13.3t with Pytorch > 2.8 for mult-gpu quantization and multi-cpu packing with env `PYTHON_GIL=0`.
[33mWARN[0m  Feature `utils/Perplexity` requires Python < 3.14 and Python GIL enabled and Python >= 3.13.3T (T for Threading-Free edition of Python) plus Torch 2.8. Feature is currently skipped/disabled.
[32mINFO[0m  ENV: Auto setting PYTORCH_ALLOC_CONF='expandable_segments:True,max_split_size_mb:256,garbage_collection_threshold:0.7' for memory saving.
[32mINFO[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          
[36mDEBUG[0m BitBLAS import failed: No module named 'bitblas'                         
[36mDEBUG[0m Skipping qlinear module import `bitblas_target_detector`: No module named 'thefuzz'
[32mINFO[0m  

_____/\\\\\\\\\\\\__/\\\\\\\\\\\\\____/\\\\\\\\\\\\\\\______________________/\\\________/\\\\____________/\\\\_______________________/\\\__________________/\\\\\\____
 ___/\\\//////////__\/\\\/////////\\\_\///////\\\/////____________________/\\\\/\\\\____\/\\\\\\________/\\\\\\______________________\/\\\_________________\////\\\____
  __/\\\_____________\/\\\_______\/\\\_______\/\\\_______________________/\\\//\////\\\__\/\\\//\\\____/\\\//\\\______________________\/\\\____________________\/\\\____
   _\/\\\____/\\\\\\\_\/\\\\\\\\\\\\\/________\/\\\________/\\\\\\\\\\\__/\\\______\//\\\_\/\\\\///\\\/\\\/_\/\\\_____/\\\\\___________\/\\\______/\\\\\\\\_____\/\\\____
    _\/\\\___\/////\\\_\/\\\/////////__________\/\\\_______\///////////__\//\\\______/\\\__\/\\\__\///\\\/___\/\\\___/\\\///\\\____/\\\\\\\\\____/\\\/////\\\____\/\\\____
     _\/\\\_______\/\\\_\/\\\___________________\/\\\______________________\///\\\\/\\\\/___\/\\\____\///_____\/\\\__/\\\__\//\\\__/\\\////\\\___/\\\\\\\\\\\_____\/\\\____
      _\/\\\_______\/\\\_\/\\\___________________\/\\\________________________\////\\\//_____\/\\\_____________\/\\\_\//\\\__/\\\__\/\\\__\/\\\__\//\\///////______\/\\\____
       _\//\\\\\\\\\\\\/__\/\\\___________________\/\\\___________________________\///\\\\\\__\/\\\_____________\/\\\__\///\\\\\/___\//\\\\\\\/\\__\//\\\\\\\\\\__/\\\\\\\\\_
        __\////////////____\///____________________\///______________________________\//////___\///______________\///_____\/////______\///////\//____\//////////__\/////////__

[32mINFO[0m  Kernel: Auto-selection: adding candidate `TritonV2QuantLinear`           
[32mINFO[0m  Kernel: selected -> `TritonV2QuantLinear`.                               
[32mINFO[0m  Optimize: `TritonV2QuantLinear` compilation triggered.                   
[32mINFO[0m  gc.collect() reclaimed 179 objects in 0.158s                             
[*] å¼€å§‹ä½¿ç”¨é¡¹ç›®çš„ UniversalEvaluator è¿›è¡Œå¯¹é½è¯„æµ‹...
[*] è¯„æµ‹ä»»åŠ¡: ['mmlu', 'gsm8k', 'ifeval']

============================================================
 ğŸ“Š GPTQ 4-bit æ¨¡å‹è¯„æµ‹ç»“æœ (å¯¹é½é¡¹ç›®æ ‡å‡†)
============================================================
 mmlu/acc            : 0.6402
 mmlu_humanities/acc : 0.5819
 mmlu_other/acc      : 0.7187
 mmlu_social_sciences/acc: 0.7364
 mmlu_stem/acc       : 0.5557
 ifeval/prompt_strict_acc: 0.4011
 ifeval/inst_strict_acc: 0.5516
 ifeval/prompt_loose_acc: 0.4381
 ifeval/inst_loose_acc: 0.5839
 gsm8k/acc           : 0.6702
 mt_bench/speed      : 20.4487
 mt_bench/saved      : 1.0000
============================================================

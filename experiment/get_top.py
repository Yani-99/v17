import json
import shutil
import tempfile
import os
from huggingface_hub import HfApi, hf_hub_download

def get_structural_fingerprint(config):
    """æå–ç»“æ„æŒ‡çº¹ (ä¸ä¹‹å‰é€»è¾‘ä¸€è‡´)"""
    fingerprint = {}
    fingerprint['vocab_size'] = config.get('vocab_size')
    fingerprint['hidden_size'] = config.get('hidden_size') or config.get('d_model') or config.get('n_embd')
    fingerprint['num_layers'] = config.get('num_hidden_layers') or config.get('n_layer') or config.get('num_layers')
    fingerprint['num_heads'] = config.get('num_attention_heads') or config.get('n_head')
    return fingerprint

def find_structurally_compatible_models_clean(base_model_name, top_k=5):
    api = HfApi()
    
    # 1. åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ç”¨äºå­˜æ”¾ä¸‹è½½çš„ config.json
    # tempfile.mkdtemp() ä¼šåœ¨ç³»ç»Ÿä¸´æ—¶åŒºåˆ›å»ºä¸€ä¸ªéšæœºå‘½åçš„æ–‡ä»¶å¤¹
    temp_cache_dir = tempfile.mkdtemp(prefix="hf_config_check_")
    print(f"ğŸ“ åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•: {temp_cache_dir}")
    print(f"   (ä»»åŠ¡ç»“æŸåå°†è‡ªåŠ¨åˆ é™¤æ­¤ç›®å½•)")

    try:
        print(f"ğŸ“Š æ­£åœ¨è·å–åŸºç¡€æ¨¡å‹ [{base_model_name}] çš„ç»“æ„æŒ‡çº¹...")
        
        # ä¸‹è½½åŸºç¡€æ¨¡å‹ config åˆ°ä¸´æ—¶ç›®å½•
        base_config_path = hf_hub_download(
            base_model_name, 
            "config.json", 
            cache_dir=temp_cache_dir  # <--- å…³é”®ç‚¹ï¼šæŒ‡å®šç¼“å­˜è·¯å¾„
        )
        with open(base_config_path, 'r') as f:
            base_config = json.load(f)
        base_fp = get_structural_fingerprint(base_config)
        print(f"   åŸºç¡€æ¨¡å‹æŒ‡çº¹: {base_fp}")

        print(f"ğŸ” æ­£åœ¨æœç´¢å¹¶æ¯”å¯¹...")
        candidates = api.list_models(
            search=base_model_name.split("/")[-1],
            sort="downloads",
            direction=-1,
            limit=100
        )
        
        valid_models = []
        
        for model in candidates:
            if model.id == base_model_name: continue
            if "whole-word-masking" in model.id.lower(): continue
            invalid_keywords = ["openai-community", "xenova", "onnx", "quantized"]
            
            is_invalid = False
            for kw in invalid_keywords:
                if kw in model.id.lower():
                    is_invalid = True
                    break
            if is_invalid:
                continue
            try:
                # ä¸‹è½½å€™é€‰æ¨¡å‹ config åˆ°ä¸´æ—¶ç›®å½•
                config_path = hf_hub_download(
                    model.id, 
                    "config.json", 
                    cache_dir=temp_cache_dir # <--- å…³é”®ç‚¹ï¼šæŒ‡å®šç¼“å­˜è·¯å¾„
                )
                
                with open(config_path, 'r') as f:
                    cand_config = json.load(f)
                
                cand_fp = get_structural_fingerprint(cand_config)
                
                if cand_fp == base_fp:
                    valid_models.append(model)
                    print(f"âœ… [åŒ¹é…] {model.id}")
            except:
                continue
                
            if len(valid_models) >= top_k:
                break
                
        return valid_models

    finally:
        # 2. æ¸…ç†å·¥ä½œï¼šæ— è®ºç¨‹åºæ˜¯å¦å‡ºé”™ï¼Œéƒ½åœ¨æœ€ååˆ é™¤ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_cache_dir):
            shutil.rmtree(temp_cache_dir)
            print(f"ğŸ§¹ å·²åˆ é™¤ä¸´æ—¶ç¼“å­˜ç›®å½•: {temp_cache_dir}")

# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # target = "gpt2"
    # target = "bert-large-uncased"
    # target = "meta-llama/Llama-2-7b-hf"
    # target = "meta-llama/Meta-Llama-3.1-8B"
    target = "meta-llama/Llama-2-7b-hf"
    
    final_list = find_structurally_compatible_models_clean(target, top_k=25)
    
    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ (æ— æ®‹ç•™æ–‡ä»¶):")
    for i, m in enumerate(final_list, 1):
        print(f"{i}. {m.id}")
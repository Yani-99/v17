-- Found pybind11: /home/liu4441/.conda/envs/liana/lib/python3.9/site-packages/pybind11/include (found version "3.0.1")
-- Configuring done (0.1s)
-- Generating done (0.0s)
-- Build files have been written to: /home/newdrive/liu4441/liana/pfor/standard/v17/build
[100%] Built target pforex_cpp
[RUN] Running benchmarks...
[CONFIG] 手动设定: 启动 1 个并行线程

>>> Running Config: vicuna <<<
Detecting FT model native dtype...

[========== Rate: 0.0 ==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running loglikelihood requests
  -> Size: 68.11% | Comp: 228.5 MB/s | Decomp: 419.5 MB/s | Metrics: {'mmlu/acc': 0.4968421052631579, 'mmlu_humanities/acc': 0.5330769230769231, 'mmlu_other/acc': 0.5361538461538462, 'mmlu_social_sciences/acc': 0.575, 'mmlu_stem/acc': 0.3957894736842105}

[========== Rate: 0.01 ==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running loglikelihood requests
  -> Size: 64.22% | Comp: 180.0 MB/s | Decomp: 382.1 MB/s | Metrics: {'mmlu/acc': 0.4964912280701754, 'mmlu_humanities/acc': 0.5353846153846153, 'mmlu_other/acc': 0.5353846153846153, 'mmlu_social_sciences/acc': 0.5741666666666667, 'mmlu_stem/acc': 0.39421052631578946}

[========== Rate: 0.05 ==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running loglikelihood requests
  -> Size: 40.52% | Comp: 129.1 MB/s | Decomp: 376.8 MB/s | Metrics: {'mmlu/acc': 0.4942105263157895, 'mmlu_humanities/acc': 0.54, 'mmlu_other/acc': 0.5284615384615384, 'mmlu_social_sciences/acc': 0.5741666666666667, 'mmlu_stem/acc': 0.38894736842105265}

[========== Rate: 0.1 ==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running loglikelihood requests
  -> Size: 17.67% | Comp: 121.9 MB/s | Decomp: 720.8 MB/s | Metrics: {'mmlu/acc': 0.49543859649122807, 'mmlu_humanities/acc': 0.5476923076923077, 'mmlu_other/acc': 0.5246153846153846, 'mmlu_social_sciences/acc': 0.5625, 'mmlu_stem/acc': 0.3973684210526316}

[========== Rate: 0.15 ==========]
We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Preparing LM-Eval Harness: ['mmlu']...
`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
Running LM-Eval (Attempt 1/3)...
Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
Using pre-initialized model
Building contexts for mmlu_abstract_algebra on rank 0...
Building contexts for mmlu_computer_security on rank 0...
Building contexts for mmlu_high_school_biology on rank 0...
Building contexts for mmlu_astronomy on rank 0...
Building contexts for mmlu_college_computer_science on rank 0...
Building contexts for mmlu_conceptual_physics on rank 0...
Building contexts for mmlu_anatomy on rank 0...
Building contexts for mmlu_high_school_computer_science on rank 0...
Building contexts for mmlu_college_physics on rank 0...
Building contexts for mmlu_college_mathematics on rank 0...
Building contexts for mmlu_electrical_engineering on rank 0...
Building contexts for mmlu_high_school_physics on rank 0...
Building contexts for mmlu_college_biology on rank 0...
Building contexts for mmlu_machine_learning on rank 0...
Building contexts for mmlu_elementary_mathematics on rank 0...
Building contexts for mmlu_high_school_mathematics on rank 0...
Building contexts for mmlu_college_chemistry on rank 0...
Building contexts for mmlu_high_school_chemistry on rank 0...
Building contexts for mmlu_high_school_statistics on rank 0...
Building contexts for mmlu_professional_medicine on rank 0...
Building contexts for mmlu_medical_genetics on rank 0...
Building contexts for mmlu_nutrition on rank 0...
Building contexts for mmlu_clinical_knowledge on rank 0...
Building contexts for mmlu_global_facts on rank 0...
Building contexts for mmlu_virology on rank 0...
Building contexts for mmlu_professional_accounting on rank 0...
Building contexts for mmlu_miscellaneous on rank 0...
Building contexts for mmlu_marketing on rank 0...
Building contexts for mmlu_college_medicine on rank 0...
Building contexts for mmlu_management on rank 0...
Building contexts for mmlu_human_aging on rank 0...
Building contexts for mmlu_business_ethics on rank 0...
Building contexts for mmlu_high_school_geography on rank 0...
Building contexts for mmlu_high_school_psychology on rank 0...
Building contexts for mmlu_public_relations on rank 0...
Building contexts for mmlu_high_school_government_and_politics on rank 0...
Building contexts for mmlu_us_foreign_policy on rank 0...
Building contexts for mmlu_professional_psychology on rank 0...
Building contexts for mmlu_econometrics on rank 0...
Building contexts for mmlu_security_studies on rank 0...
Building contexts for mmlu_high_school_macroeconomics on rank 0...
Building contexts for mmlu_sociology on rank 0...
Building contexts for mmlu_human_sexuality on rank 0...
Building contexts for mmlu_high_school_microeconomics on rank 0...
Building contexts for mmlu_prehistory on rank 0...
Building contexts for mmlu_professional_law on rank 0...
Building contexts for mmlu_international_law on rank 0...
Building contexts for mmlu_moral_disputes on rank 0...
Building contexts for mmlu_logical_fallacies on rank 0...
Building contexts for mmlu_formal_logic on rank 0...
Building contexts for mmlu_world_religions on rank 0...
Building contexts for mmlu_philosophy on rank 0...
Building contexts for mmlu_high_school_european_history on rank 0...
Building contexts for mmlu_moral_scenarios on rank 0...
Building contexts for mmlu_jurisprudence on rank 0...
Building contexts for mmlu_high_school_us_history on rank 0...
Building contexts for mmlu_high_school_world_history on rank 0...
Running loglikelihood requests
  -> Size: 4.28% | Comp: 164.2 MB/s | Decomp: 637.5 MB/s | Metrics: {'mmlu/acc': 0.46596491228070175, 'mmlu_humanities/acc': 0.5261538461538462, 'mmlu_other/acc': 0.4969230769230769, 'mmlu_social_sciences/acc': 0.5191666666666667, 'mmlu_stem/acc': 0.37}

====================================================================================================
 FINAL SUMMARY: vicuna (LLM_HARNESS) 
====================================================================================================
Rate     | Comp %     | C-Speed      | D-Speed      | Main Metric
-----------------------------------------------------------------
0.0      | 68.11%     | 228.5 MB/s   | 419.5 MB/s   | mmlu/acc: 0.4968, mmlu_humanities/acc: 0.5331
====================================================================================================
0.01     | 64.22%     | 180.0 MB/s   | 382.1 MB/s   | mmlu/acc: 0.4965, mmlu_humanities/acc: 0.5354
====================================================================================================
0.05     | 40.52%     | 129.1 MB/s   | 376.8 MB/s   | mmlu/acc: 0.4942, mmlu_humanities/acc: 0.5400
====================================================================================================
0.1      | 17.67%     | 121.9 MB/s   | 720.8 MB/s   | mmlu/acc: 0.4954, mmlu_humanities/acc: 0.5477
====================================================================================================
0.15     | 4.28%      | 164.2 MB/s   | 637.5 MB/s   | mmlu/acc: 0.4660, mmlu_humanities/acc: 0.5262
====================================================================================================
